learning_rate: 0.001
optimizer: adam
batch_size: 32
weight_decay: 0.0001
num_epochs: 1000
eval_every: 50
early_stopping_patience: 20
